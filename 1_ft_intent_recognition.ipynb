{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60f9208d-dee4-433d-b9c1-8e52976d118a",
   "metadata": {},
   "source": [
    "# Fine-Tune Intent Recognition Model in LoRA way "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc9e414f-92f4-4102-abb6-55010cb6e76e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\.conda\\envs\\py310torch\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#Libraries\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from shutil import unpack_archive\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f07607cd-442d-4080-99e1-3ca92985be26",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_columns\", 101)\n",
    "pd.set_option('display.max_colwidth', 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a3459b-a86d-4acd-8a00-ee7b51d0aac4",
   "metadata": {},
   "source": [
    "### 1. Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6d071c93-f4e4-44af-90cf-796ef905acef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instruction</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i dont have a user account can ya help me open it</td>\n",
       "      <td>Bank</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I'd like to close a user account, where to do it?</td>\n",
       "      <td>Bank</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I'm trying to find informayion about the current balance of my account</td>\n",
       "      <td>Bank</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i dotn wanna keep my fucking account help me close it</td>\n",
       "      <td>Bank</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i got to close a fucking user account how to do it</td>\n",
       "      <td>Bank</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>I have to book a flight ticket Nonbank, how can I do it?</td>\n",
       "      <td>Nonbank</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>I have to cancel a subscription Nonbank, how can I do it?</td>\n",
       "      <td>Nonbank</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>I have to set up a new email account Nonbank, how can I do it?</td>\n",
       "      <td>Nonbank</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>I have to learn a new language Nonbank, how can I do it?</td>\n",
       "      <td>Nonbank</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>I have to pay my bills online Nonbank, how can I do it?</td>\n",
       "      <td>Nonbank</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>199 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                instruction  \\\n",
       "0                         i dont have a user account can ya help me open it   \n",
       "1                         I'd like to close a user account, where to do it?   \n",
       "2    I'm trying to find informayion about the current balance of my account   \n",
       "3                     i dotn wanna keep my fucking account help me close it   \n",
       "4                        i got to close a fucking user account how to do it   \n",
       "..                                                                      ...   \n",
       "194                I have to book a flight ticket Nonbank, how can I do it?   \n",
       "195               I have to cancel a subscription Nonbank, how can I do it?   \n",
       "196          I have to set up a new email account Nonbank, how can I do it?   \n",
       "197                I have to learn a new language Nonbank, how can I do it?   \n",
       "198                 I have to pay my bills online Nonbank, how can I do it?   \n",
       "\n",
       "    category  \n",
       "0       Bank  \n",
       "1       Bank  \n",
       "2       Bank  \n",
       "3       Bank  \n",
       "4       Bank  \n",
       "..       ...  \n",
       "194  Nonbank  \n",
       "195  Nonbank  \n",
       "196  Nonbank  \n",
       "197  Nonbank  \n",
       "198  Nonbank  \n",
       "\n",
       "[199 rows x 2 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(os.path.join(\"datasets\",\"generated_inquiries.csv\"))\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "83ca5b16-bbb3-4077-bc75-ff98512a67f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change label to binary\n",
    "data.loc[data['category'] == 'Nonbank', 'category'] = 0\n",
    "data.loc[data['category'] == 'Bank', 'category'] = 1\n",
    "data['category'] = data['category'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bac6714a-de85-4a2f-b2a1-a37e9204865d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instruction</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i dont have a user account can ya help me open it</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I'd like to close a user account, where to do it?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I'm trying to find informayion about the current balance of my account</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i dotn wanna keep my fucking account help me close it</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i got to close a fucking user account how to do it</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                              instruction  \\\n",
       "0                       i dont have a user account can ya help me open it   \n",
       "1                       I'd like to close a user account, where to do it?   \n",
       "2  I'm trying to find informayion about the current balance of my account   \n",
       "3                   i dotn wanna keep my fucking account help me close it   \n",
       "4                      i got to close a fucking user account how to do it   \n",
       "\n",
       "   category  \n",
       "0         1  \n",
       "1         1  \n",
       "2         1  \n",
       "3         1  \n",
       "4         1  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2e52fb86-4d4e-491a-bb69-3959a9b898ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "category\n",
       "0    100\n",
       "1     99\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# samples are balanced\n",
    "data['category'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e668d251-ec65-49f0-839a-f7276cd0c115",
   "metadata": {},
   "source": [
    "### 2. Tokenization with Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24ff249a-1ef4-4bd6-bb48-a7ebea288b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "PreTrained_Model = 'bert-base-uncased'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1dc3bf9-3a74-4b5f-9772-2d4470c91594",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\.conda\\envs\\py310torch\\lib\\site-packages\\huggingface_hub\\file_download.py:142: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Administrator\\.cache\\huggingface\\hub\\models--bert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(PreTrained_Model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee576ab-5197-4843-8356-ec874c7d9160",
   "metadata": {},
   "source": [
    "### 3. Prepare Training/test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "912cf254-a7d2-468d-933b-ff710854a5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build dataset\n",
    "class NewsDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        text = str(self.texts[item])\n",
    "        label = self.labels[item]\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,  # Add '[CLS]' and '[SEP]'\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0f86af2e-82ae-4e0f-ba25-788c6fd690f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 128\n",
    "BATCH_SIZE = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "04158898-f37a-40ba-8262-7e47f0364d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts, val_texts, train_labels, val_labels = train_test_split(data['instruction'].to_numpy(), data['category'].to_numpy(), test_size=0.2)\n",
    "\n",
    "train_dataset = NewsDataset(train_texts, train_labels, tokenizer, MAX_LEN)\n",
    "val_dataset = NewsDataset(val_texts, val_labels, tokenizer, MAX_LEN)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "02faff32-6a6e-442a-84f2-2130f74fffcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,\n",
       "       1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0,\n",
       "       0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0,\n",
       "       0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1,\n",
       "       1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0,\n",
       "       0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1,\n",
       "       1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1,\n",
       "       0, 1, 1, 0, 0])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ad5180-b765-4d8e-af3a-e0033d6bc465",
   "metadata": {},
   "source": [
    "### 4. Load Pre-Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fb48916b-5349-4ba6-aa30-ca68ee490708",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3d562e72-a4d3-4be3-9175-2a6eb2bca4f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "# Load the pre-trained BERT model\n",
    "model = BertForSequenceClassification.from_pretrained(PreTrained_Model, num_labels=2)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "# Freeze BERT parameters\n",
    "for param in model.base_model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca9eb50-8b40-4cf9-b324-4b4326a1b929",
   "metadata": {},
   "source": [
    "### 5. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "9721b9ea-ca36-4f0f-802c-c137e91de352",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\.conda\\envs\\py310torch\\lib\\site-packages\\peft\\mapping_func.py:73: UserWarning: You are trying to modify a model with PEFT for a second time. If you want to reload the model with a different config, make sure to call `.unload()` before.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Administrator\\.conda\\envs\\py310torch\\lib\\site-packages\\peft\\tuners\\tuners_utils.py:167: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n",
      "C:\\Users\\Administrator\\.conda\\envs\\py310torch\\lib\\site-packages\\transformers\\training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Set up LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS,  # Define the task type\n",
    "    r=8,  # Rank of low-rank matrices (you can adjust based on your needs)\n",
    "    lora_alpha=16,  # Scaling factor (can be adjusted)\n",
    "    lora_dropout=0.1,  # Dropout for LoRA layers\n",
    "    bias=\"none\"  # Specify whether to include bias terms in the low-rank matrices\n",
    ")\n",
    "\n",
    "# Get the LoRA-enhanced model\n",
    "model_with_lora = get_peft_model(model, lora_config)\n",
    "\n",
    "# Set up training arguments and Trainer\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=50,\n",
    "    weight_decay=0.01,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "a50d7bad-41d4-4298-835d-1156b5cb3625",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_29092\\1510435588.py:2: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [500/500 00:31, Epoch 50/50]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.543273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.531473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.519953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.505856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.489199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.472114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.454607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.437539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.420972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.403324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.385329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.367623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.349680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.335468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.322090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.307532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.292902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.277647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.265755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.254926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.244156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.231422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.218393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.209008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.200476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.194468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.189735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.184392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.176348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.169890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.165591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.161895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.155932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.152821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.150064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.144331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.141878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.137998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.136783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.133711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.131901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.131319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.131200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.128754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.126174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.123510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.121918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.120714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.120283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.256500</td>\n",
       "      <td>0.120176</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=500, training_loss=0.2565166015625, metrics={'train_runtime': 31.6923, 'train_samples_per_second': 250.849, 'train_steps_per_second': 15.777, 'total_flos': 524743227648000.0, 'train_loss': 0.2565166015625, 'epoch': 50.0})"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define Trainer for evaluation\n",
    "trainer = Trainer(\n",
    "    model=model_with_lora,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ffba85-851b-408f-b9db-8f04d30aebf4",
   "metadata": {},
   "source": [
    "### 5. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "c13a2f43-ea31-4832-98e2-b04919f4f805",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "7d33ce01-b680-4205-b272-a29afe8aeb7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, val_dataloader):\n",
    "    model.eval()\n",
    "    total_correct = 0\n",
    "    total_count = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            predictions = torch.argmax(outputs.logits, dim=1)\n",
    "            total_correct += (predictions == labels).sum().item()\n",
    "            total_count += labels.size(0)\n",
    "\n",
    "    accuracy = total_correct / total_count\n",
    "    f1 = f1_score(all_labels, all_predictions, average='binary')  # 'binary' for 2-class classification\n",
    "\n",
    "    print(f\"Validation accuracy: {accuracy * 100:.2f}%\")\n",
    "    print(f\"Validation F1 Score: {f1:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "7b0c5cfd-0459-4beb-ab00-6beb68df8d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "total_correct = 0\n",
    "total_count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "673cbe11-7c88-4f97-b958-b62847dba5a2",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'attention_ma sk'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[90], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m val_dataloader:\n\u001b[0;32m      3\u001b[0m     input_ids \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m----> 4\u001b[0m     attention_mask \u001b[38;5;241m=\u001b[39m \u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mattention_ma sk\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      5\u001b[0m     labels \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      7\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(input_ids\u001b[38;5;241m=\u001b[39minput_ids, attention_mask\u001b[38;5;241m=\u001b[39mattention_mask)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'attention_ma sk'"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    for batch in val_dataloader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_ma sk'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        predictions = torch.argmax(outputs.logits, dim=1)\n",
    "        total_correct += (predictions == labels).sum().item()\n",
    "        total_count += labels.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "3d06a034-375f-4002-9552-bb268aab5ee9",
   "metadata": {},
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[91], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mtotal_correct\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtotal_count\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy\u001b[38;5;250m \u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m100\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "accuracy = total_correct / total_count\n",
    "print(f\"Validation accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbad94a9-be67-4634-8a34-16fbc854e7a5",
   "metadata": {},
   "source": [
    "### 6. Model Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "9e60e280-92db-4047-bf0d-f9177947681e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, texts, tokenizer):\n",
    "    model.eval()\n",
    "    encodings = tokenizer(texts, padding=True, truncation=True, return_tensors='pt')\n",
    "    input_ids = encodings['input_ids'].to(device)\n",
    "    attention_mask = encodings['attention_mask'].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        predictions = torch.argmax(outputs.logits, dim=1)\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "9d80b345-c3ec-4cb4-b015-382e8b38f40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred_list = []\n",
    "\n",
    "for text in train_texts:\n",
    "    pred = predict(model_with_lora, text, tokenizer)\n",
    "    train_pred_list.append(predictions.tolist()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "a3897b96-d014-499b-858f-2b25a8371e51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of training dataset: 0.5094\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(train_labels, train_pred_list)\n",
    "print(f'Accuracy of training dataset: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "eeaec4fe-919f-4322-816d-8d76f31504cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score of training dataset: 0.00\n"
     ]
    }
   ],
   "source": [
    "f1 = f1_score(train_labels, train_pred_list, average='binary')  # 'binary' for 2-class classification\n",
    "print(f\"F1 Score of training dataset: {f1:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "27cc3412-770c-4146-9735-b3c820cd5058",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1,\n",
       "       1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0,\n",
       "       0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0,\n",
       "       0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1,\n",
       "       1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0,\n",
       "       0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1,\n",
       "       1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1,\n",
       "       0, 1, 1, 0, 0])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef0bd56-f505-48c1-bd93-f5fb49202d5d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
